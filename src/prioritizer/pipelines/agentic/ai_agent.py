import pandas as pd

from langchain.tools import tool
from langchain_ollama import ChatOllama
from langchain_core.documents import Document
from langchain.agents import create_agent
from langchain.messages import AIMessage
from langchain_core.runnables import RunnableLambda
from langgraph.checkpoint.memory import InMemorySaver



from history.git_history import build_report

from typing import List, Optional
from git import Repo

def extract_final_content(state: dict) -> str:
    """
    Try to extract the most relevant AI-produced content from a LangGraph state.

    Strategy:
    1. Prefer the last AIMessage with non-empty `content`.
    2. If none exists, fall back to the last AIMessage (even if empty) and
       show debug info.
    3. If there is no AIMessage at all, show the raw messages for debugging.
    """
    messages = state.get("messages", [])
    if not messages:
        return "[DEBUG] No messages in state."

    # 1) Last AIMessage with non-empty content
    for msg in reversed(messages):
        if isinstance(msg, AIMessage) and getattr(msg, "content", None):
            return msg.content

    # 2) Fallback: last AIMessage even if empty
    for msg in reversed(messages):
        if isinstance(msg, AIMessage):
            return f"[DEBUG] AIMessage with empty content: {msg!r}"

    # 3) No AIMessage at all
    return f"[DEBUG] No AIMessage found. Messages: {messages!r}"
    

@tool
def read_relevant_code_smells_and_write_to_lang_chain_documents(smell_filter: List[str]) -> List[Document]:
    """
    Retrieve and convert relevant code smells from the CSV report into LangChain Document objects.

    This function:
      - Reads the static code quality report (CSV) generated by python-smells-detector.
      - Filters the code smells to only include those listed in `smell_filter`.
      - Builds a formatted textual overview for each selected code smell.
      - Wraps each smell entry into a LangChain Document with useful metadata.

    Args:
        smell_filter (List[str]):
            A list of smell names to include (e.g., ["Long Method", "Feature Envy"]).

    Returns:
        List[Document]:
            A list of LangChain Document objects, where each document contains
            a formatted code smell report + metadata fields like:
              - type (str)
              - smell_name (str)
              - file (str)
              - module (str)
    """
    df = pd.read_csv("python_smells_detector/code_quality_report.csv")
    docs: List[Document] = []

    i = 1


    for _, row in df.iterrows():
        if row["Name"] not in smell_filter:
            continue
        
        code_smell = (
            f"Code smell {i}\n"
            f"Type of smell: {row['Type']}\n"
            f"Name: {row['Name']}\n"
            f"File_path: {row['File']}\n"
            f"Module/class: {row['Module/Class']}\n"
            f"Line number: {row['Line Number']}\n"
            f"Description: {row['Description']}\n\n"
        )
            
        docs.append(Document(page_content=code_smell, meta={
        "type": row["Type"],
        "smell_name": row["Name"],
        "file_path": row["File"],
        "module": row["Module/Class"],
        "line number": row["Line Number"],
        }))

        i += 1


    return docs

@tool
def get_git_statistics_from_file(project_name: str, file_path) -> str:
    """
    Retrieve Git-based evolutionary metrics for a file impacted by a code smell.

    This tool:
      - Opens the project's Git repository under `projects/<project_name>`
      - Extracts change history specifically for the target file
      - Returns evolution metrics useful for technical debt prioritization, such as:
          * number of commits touching the file
          * number of bug-fix commits
          * churn (added/removed LOC)
          * recency of last modification
          * contributor count

    These metrics help the agent assess refactoring urgency based on:
      - Change-proneness
      - Fault-proneness
      - Propagation risk
      - Long-term maintenance cost

    Args:
        project_name (str):
            The folder name under 'projects/' where the Git repository is stored.
            Example: "gitmetrics"
        file_path (str):
            Full or relative path to the smell's file. The filename component is
            automatically extracted before querying Git.

    Returns:
        str:
            A Git analysis report generated by `build_report()`, structured for direct
            use by the prioritization and refinement agent.

    Notes:
        - Only the filename is used to detect history; ensure uniqueness within the repo.
        - Designed to support multi-signal technical debt scoring (code + history).
    """
    project_name="gitmetrics"
    git_project_repo = Repo(f"projects/{project_name}")

    print(git_project_repo, file_path)

    return build_report(git_project_repo, file_path.split("/")[-1])

@tool
def get_pylint_and_astroid_statistical_analysis_report(file_path: str) -> str:
    """
    Execute pylint- and astroid-based static analysis for the target file, returning
    a concise LLM-friendly report about potential code quality issues.

    This tool:
      - Normalizes the file path to ensure it refers to a valid project location.
      - Reuses a cached Pylint + Reporter instance if available, which significantly
        reduces the overhead of repeated linting operations by avoiding reloading
        configuration and AST introspection plugins.
      - Produces a plain-text summary (warning categories, error counts, maintainability
        indicators, etc.) designed to support technical debt assessment.

    Args:
        file_path (str):
            Relative or absolute path to the file being analyzed. Paths starting
            with '../' are automatically rewritten so the tool resolves properly
            within the local project.

    Returns:
        str:
            A structured textual linting report describing:
                * defect-prone constructs (e.g., unused variables, dead code)
                * complexity indicators and maintainability warnings
                * potential design issues from AST analysis

    Notes:
        - This tool is intended to help an agent refine prioritization scores by
          incorporating static code quality evidence.
        - Only a condensed summary is returned; verbose lint outputs are avoided
          to stay within LLM token limits.
    """
    if file_path.startswith("../"):
        file_path = file_path[3:]



    return build_llm_analysis_report(file_path)["text"]

@tool
def get_code_segment_from_file(line_number: float, file_path: str) -> Optional[str]:
    """
    Extract a relevant source code segment around a code smell location.

    This tool reads a Python file from disk and returns the function,
    class, or code block that starts at the specified line number.
    The AST is used to determine the entity boundaries so the returned
    snippet includes the complete definition rather than only a single line.

    Args:
        line_number (float):
            1-based line number where the code smell was detected.
            If NaN, returns the whole file content.
        file_path (str):
            Path to the Python file containing the code smell.

    Returns:
        Optional[str]:
            A formatted substring of the source code corresponding to
            a function/class starting at the provided line number.
            Returns None if no corresponding entity is found.

    Notes:
        - The file path is normalized to an absolute path before reading.
        - Intended for LLM-based refactoring and prioritization tasks where
          the agent may use specific code evidence when scoring smells.
    """
    if file_path.startswith("../"):
        file_path = file_path[3:]

    return get_code_segment_from_file_based_on_line_number(line_number, file_path)

def main():
    llm = ChatOllama(
        model="gpt-oss:20b-cloud",
        validate_model_on_init=True,
        temperature=0,
    )

    smell_filter = ['Long Method', 'Large Class', 'Long File', 'High Cyclomatic Complexity', 'Feature Envy']

    system_prompt = f"""
    You are an expert software engineer helping to analyze and prioritize code smells in a project.

    First, you MUST call the tool
    `read_relevant_code_smells_and_write_to_lang_chain_documents`
    with the argument:

    smell_filter = {smell_filter}

    Use the string returned by that tool as your ONLY source of information about
    the available code smells.

    Then:
    1. Parse the code smell overview that the tool returned.
    2. For each smell, assign a priority score from 1 to 20
    (1 = very low priority to refactor, 20 = extremely high priority).
    3. Produce a final JSON object with this structure:

    {{
    "ranked_smells": [
        {{
        "index": <int>,            // the "Code smell X" index from the overview
        "name": <str>,             // smell name
        "file": <str>,             // file path
        "module": <str>,           // module/class info
        "score": <int>,            // 1-20
        "reason": <str>            // short explanation for the score
        }},
        ...
    ]
    }}

    Your final answer must be ONLY this JSON object, nothing else.
    """

    tools = [
        read_relevant_code_smells_and_write_to_lang_chain_documents, 
        get_code_segment_from_file, 
        get_git_statistics_from_file,
        #get_pylint_and_astroid_statistical_analysis_report,
    ]

    checkpointer = InMemorySaver()

    agent = create_agent(
        model=llm, 
        tools=[get_git_statistics_from_file], 
        system_prompt=system_prompt, 
    ) | RunnableLambda(extract_final_content)

    user_prompt = """
    I have a bunch of code smells that I need to search for in a file. Can you find them and generate a simple overview
    and assign each a priority score between 1 and 20 as described?
    """

    config = {"configurable": {"thread_id": "smell-prioritizer-1"}}

    initial_ranking = agent.invoke(
        {
        "messages": [
            {"role": "user", "content": user_prompt}
        ]},
    )

    print("Initial ranking:", initial_ranking)

if __name__ == "__main__":
    main()
